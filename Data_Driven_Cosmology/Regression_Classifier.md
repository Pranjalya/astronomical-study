## Regression Classifier

* We will be using flux magnitudes from the Sloan Digital Sky Survey (SDSS) catalogue to create colour indices. Flux magnitudes are the total flux (or light) received in five frequency bands (u, g, r, i and z).

*  The astronomical colour (or colour index) is the difference between the magnitudes of two filters, i.e. u - g or i - z.

* This index is one way to characterise the colours of galaxies. For example, if the u-g index is high then the object is brighter in ultra violet frequencies than it is in visible green frequencies.

* Colour indices act as an approximation for the spectrum of the object and are useful for classifying stars into different types. 

*  To calculate the redshift of a distant galaxy, the most accurate method is to observe the optical emission lines and measure the shift in wavelength. However, this process can be time consuming and is thus infeasible for large samples.

* For many galaxies we simply don't have spectroscopic observations.

* Instead, we can calculate the redshift by measuring the flux using a number of different filters and comparing this to models of what we expect galaxies to look like at different redshifts.

* We will use machine learning to obtain photometric redshifts for a large sample of galaxies. We will use the colour indices (u-g, g-i, r-i and i-z) as our input and a subset of sources with spectroscopic redshifts as the training dataset.

### Decision Tree Algorithm

*  Decision trees are a tool that can be used for both classification and regression. In this module we will look at regression, however, in the next module we will see how they can be used as classifiers.

* Decision trees map a set of input features to their corresponding output targets. This is done through a series of individual decisions where each decision represents a node (or branching) of the tree.

* The following figure shows the decision tree our proverbial robot tennis player Robi used in the lectures to try and decide whether to play tennis on a particular day. 

* Each node represents a decision that the robot needs to make (or assess) to reach a final decision. In this example, the decision tree will be passed a set of input features (Outlook, Humidity and Wind) and will return an output of whether to play or not. 

* In decision trees for real-world tasks, each decision is typically more complex, involving measured values, not just categories.

* Instead of the input values for humidity being Normal or High and wind being Strong or Weak we might see a percentage between 0 and 100 for humidity and a wind speed in km/hr for wind. Our decisions might then be humidity < 40% or wind < 5 km/hr.

* The output of regression is a real number. So, instead of the two outputs of Play and Don't Play we have a probability of whether we will play that day.

* The decision at each branch is determined from the training data by the decision tree learning algorithm. Each algorithm employs a different metric (e.g. Gini impurity or information gain) to find the decision that splits the data most effectively.

* For now, just need to know that a decision tree is a series of decisions, each made on a single feature of the data. The end point of all the branches is a set of desired target values. 

#### Decision Tree for Red Shift

* The inputs to our decision tree are the colour indices from photometric imaging and our output is a photometric redshift. Our training data uses accurate spectroscopic measurements.

* The decision tree will look something like the following. 

* We can see how our calculated colour indices are input as features at the top and through a series of decision nodes a target redshift value is reached and output. 

* So we trained a decision tree! Great...but how do we know if the tree is actually any good at predicting redshifts?

* In regression we compare the predictions generated by our model with the actual values to test how well our model is performing. The difference between the predicted values and actual values (sometimes referred to as residuals) can tell us a lot about where our model is performing well and where it is not.

* While there are a few different ways to characterise these differences, in this tutorial we will use the median of the differences between our predicted and actual values. This is given by:

\[ med_diff = median(|Y(i,pred) − Y(i,act)|) \]

Where || denotes the absolute value of the difference. 

*  This gives an unrealistic estimate of how accurate the model will be on previously unseen galaxies because the model has been optimised to get the best results on the training data.

* The simplest way to solve this problem is to split our data into training and testing subsets:

```
# initialise and train the decision tree

dtr = DecisionTreeRegressor()

dtr.fit(train_features, train_targets)

​

# get a set of prediction from the test input features

predictions = dtr.predict(test_features)

​

# compare the accuracy of the pediction againt the actual values

print(calculate_rmsd(predictions, test_targets))
```

* This method of validation is the most basic approach to validation and is called held-out validation. We will use the med_diff accuracy measure and hold-out validation in the next problem to assess the accuracy of our decision tree. 

* The median of differences of (approx) 0.02. This means that half of our galaxies have a error in the prediction of <0.02, which is pretty good. One of the reason we chose the median of differences as our accuracy measure is that it gives a fair representation of the errors especially when the distribution of errors is skewed. The graph below shows the distribution of residuals (differences) for our model along with the median and interquartile values.  
We can tell the distribution is very skewed.

#### Effect of Training Set Size

* The number of galaxies we use to train the model has a big impact on how accurate our predictions will be. This is the same with most machine learning methods: the more data that they are trained with, the more accurate their prediction will be.

*  Understanding how the accuracy of the model changes with sample size is important to understanding the limitations of our model. We are approaching the accuracy limit of the decision tree model (for our redshift problem) with a training sample size of 25,000 galaxies.

* The only way we could further improve our model would be to use more features. This might include more colour indices or even the errors associated with the measured flux magnitudes. 

### Before Machine Learning 

* Before machine learning, we would have tried to solve this problem with regression — by constructing an empirical model to predict how the dependent variable (redshift) varies with one or more independent variables (the colour indices).

* A plot of how the colours change with redshift tells us that it is quite a complex function, for example redshift versus u - g:
Redshift vs Colour-Index (u-g)

* One approach would be to construct a multi-variate non-linear regression model. Perhaps using a least squares fitting to try and determine the best fit parameters. The model would be quite complex; based on the above plot, a dampened inverse sine function would be a good starting point for such a model.

* While we could try such an approach the function would be overly complex and there is no guarantee that it would yield very promising results. Another approach would be to plot a colour-index vs colour-index plot using an additional colour scale to show redshift. The following plot is an example of such a plot.
Colour index vs colour index vs redshift plot

* It shows that we get reasonably well defined regions where redshifts are similar. If we were to make a contour map of the redshifts in the colour index vs colour index space we would be able to get an estimate of the redshift for new data points based on a combination of their colour indices. This would lead to redshift estimates with significant uncertainties attached to them. 